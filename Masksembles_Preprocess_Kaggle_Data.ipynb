{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Masksembles_Preprocess_Kaggle_Data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0482XPXIdxFj"
      },
      "source": [
        "## install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrpn5UV2BYLF",
        "outputId": "865a2085-69ca-402c-f281-433e9027bc86"
      },
      "source": [
        "!pip install turicreate"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting turicreate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/9f/a76acc465d873d217f05eac4846bd73d640b9db6d6f4a3c29ad92650fbbe/turicreate-6.4.1-cp37-cp37m-manylinux1_x86_64.whl (92.0MB)\n",
            "\u001b[K     |████████████████████████████████| 92.0MB 1.2MB/s eta 0:00:01\u001b[?25hRequirement already satisfied: requests>=2.9.1 in /usr/local/lib/python3.7/dist-packages (from turicreate) (2.23.0)\n",
            "Collecting resampy==0.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/b6/66a06d85474190b50aee1a6c09cdc95bb405ac47338b27e9b21409da1760/resampy-0.2.1.tar.gz (322kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 35.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from turicreate) (1.4.1)\n",
            "Collecting tensorflow<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/b3/3eeae9bc44039ceadceac0c7ba1cc8b1482b172810b3d7624a1cad251437/tensorflow-2.0.4-cp37-cp37m-manylinux2010_x86_64.whl (86.4MB)\n",
            "\u001b[K     |████████████████████████████████| 86.4MB 48kB/s \n",
            "\u001b[?25hCollecting numba<0.51.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/be/8c88cee3366de2a3a23a9ff1a8be34e79ad1eb1ceb0d0e33aca83655ac3c/numba-0.50.1-cp37-cp37m-manylinux2014_x86_64.whl (3.6MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6MB 32.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from turicreate) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from turicreate) (1.19.5)\n",
            "Requirement already satisfied: pandas>=0.23.2 in /usr/local/lib/python3.7/dist-packages (from turicreate) (1.1.5)\n",
            "Collecting prettytable==0.7.2\n",
            "  Downloading https://files.pythonhosted.org/packages/ef/30/4b0746848746ed5941f052479e7c23d2b56d174b82f4fd34a25e389831f5/prettytable-0.7.2.tar.bz2\n",
            "Collecting coremltools==3.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/1d/b1a99beca7355b6a026ae61fd8d3d36136e5b36f13e92ec5f81aceffc7f1/coremltools-3.3-cp37-none-manylinux1_x86_64.whl (3.5MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5MB 27.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from turicreate) (1.15.0)\n",
            "Requirement already satisfied: decorator>=4.0.9 in /usr/local/lib/python3.7/dist-packages (from turicreate) (4.4.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->turicreate) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->turicreate) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->turicreate) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->turicreate) (1.24.3)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (0.8.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (3.3.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (0.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (1.34.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (0.36.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (1.12.1)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 26.2MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 32.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (1.1.0)\n",
            "Collecting h5py<=2.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/c0/abde58b837e066bca19a3f7332d9d0493521d7dd6b48248451a9e3fe2214/h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 24.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (3.17.3)\n",
            "Collecting llvmlite<0.34,>=0.33.0.dev0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/28/0a35b3c2685bf2ea327cef5577bdf91f387f0f4594417a2a05a1d42fb7c2/llvmlite-0.33.0-cp37-cp37m-manylinux1_x86_64.whl (18.3MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3MB 172kB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba<0.51.0->turicreate) (57.0.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.2->turicreate) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.2->turicreate) (2.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (0.4.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (1.32.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (4.6.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (0.2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (3.7.4.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (0.4.8)\n",
            "Building wheels for collected packages: resampy, prettytable, gast\n",
            "  Building wheel for resampy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for resampy: filename=resampy-0.2.1-cp37-none-any.whl size=320858 sha256=9bd30be8338b765f34ec025c3f2838fd524826e1e587ff39ddb74e41866a7de3\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/4f/ed/2e6c676c23efe5394bb40ade50662e90eb46e29b48324c5f9b\n",
            "  Building wheel for prettytable (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for prettytable: filename=prettytable-0.7.2-cp37-none-any.whl size=13716 sha256=05db6b815fd6cb16fb72034b10711e6d81aac7e6a6f0ad04051f9f85dbb77195\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/34/1c/3967380d9676d162cb59513bd9dc862d0584e045a162095606\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7557 sha256=1f3ee7eea642b6ebdf2d166d04cb7034fe249307f325d1a11127a7694361519b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built resampy prettytable gast\n",
            "\u001b[31mERROR: tensorflow 2.0.4 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.13.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: librosa 0.8.1 has requirement resampy>=0.2.2, but you'll have resampy 0.2.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: llvmlite, numba, resampy, gast, h5py, keras-applications, tensorboard, tensorflow-estimator, tensorflow, prettytable, coremltools, turicreate\n",
            "  Found existing installation: llvmlite 0.34.0\n",
            "    Uninstalling llvmlite-0.34.0:\n",
            "      Successfully uninstalled llvmlite-0.34.0\n",
            "  Found existing installation: numba 0.51.2\n",
            "    Uninstalling numba-0.51.2:\n",
            "      Successfully uninstalled numba-0.51.2\n",
            "  Found existing installation: resampy 0.2.2\n",
            "    Uninstalling resampy-0.2.2:\n",
            "      Successfully uninstalled resampy-0.2.2\n",
            "  Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Found existing installation: tensorboard 2.5.0\n",
            "    Uninstalling tensorboard-2.5.0:\n",
            "      Successfully uninstalled tensorboard-2.5.0\n",
            "  Found existing installation: tensorflow-estimator 2.5.0\n",
            "    Uninstalling tensorflow-estimator-2.5.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
            "  Found existing installation: tensorflow 2.5.0\n",
            "    Uninstalling tensorflow-2.5.0:\n",
            "      Successfully uninstalled tensorflow-2.5.0\n",
            "  Found existing installation: prettytable 2.1.0\n",
            "    Uninstalling prettytable-2.1.0:\n",
            "      Successfully uninstalled prettytable-2.1.0\n",
            "Successfully installed coremltools-3.3 gast-0.2.2 h5py-2.10.0 keras-applications-1.0.8 llvmlite-0.33.0 numba-0.50.1 prettytable-0.7.2 resampy-0.2.1 tensorboard-2.0.2 tensorflow-2.0.4 tensorflow-estimator-2.0.1 turicreate-6.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2n8zfaGdp5w"
      },
      "source": [
        "## imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnCBbU95BuQ0"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import turicreate as tc\n",
        "from PIL import Image\n",
        "from numpy import asarray\n",
        "from tensorflow import keras\n",
        "import os\n",
        "from collections import Counter"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq7KjJ1Ddf65"
      },
      "source": [
        "## kaggle data API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VERXqxkMVJ2b",
        "outputId": "37fd6e16-ab2c-4734-f05b-833dc6047800"
      },
      "source": [
        "# Installing the Kaggle package\n",
        "!pip install kaggle \n",
        "!mkdir /root/.kaggle/\n",
        "!mkdir ./datasets\n",
        "\n",
        "#Important Note: complete this with your own key - after running this for the first time remmember to **remove** your API_KEY\n",
        "api_token = {\"username\":\"idoefroni\",\"key\":\"fc35681504a8107575c7a50c67f504cc\"}\n",
        "\n",
        "# creating kaggle.json file with the personal API-Key details \n",
        "# You can also put this file on your Google Drive\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "  json.dump(api_token, file)\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6pbw_R4cqGQ"
      },
      "source": [
        "## cast_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NDQC8aOawCh"
      },
      "source": [
        "this function receive dataframe with 2 columns (path,code) and return a two list of the reformeted image and the label "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_rfSlaYTkqk"
      },
      "source": [
        "def cast_img(list_img_path):\n",
        "  list_rgb = []\n",
        "  basewidth = 32\n",
        "  # load the image and convert into numpy array\n",
        "  for row in list_img_path:\n",
        "    img = Image.open(row)\n",
        "    #resize\n",
        "    img = img.resize((basewidth,basewidth), Image.ANTIALIAS)\n",
        "    numpydata = asarray(img)\n",
        "    list_rgb.append(numpydata)\n",
        "  return list_rgb"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9tIwXAORQGU"
      },
      "source": [
        "def balance_data(data_unbalanced):\n",
        "  g = data_unbalanced.groupby('label')\n",
        "  data_unbalanced = g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True))\n",
        "  data_unbalanced = data_unbalanced.reset_index(drop=True)\n",
        "  return data_unbalanced"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7CANIQ8eNKa"
      },
      "source": [
        "## get_monkey_data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVKYsCRuf9HJ"
      },
      "source": [
        "def get_monkey_data():\n",
        "  # monky dataset load and pre process \n",
        "  !mkdir ./datasets/10-monkey-species\n",
        "  # download the dataset from Kaggle and unzip it\n",
        "  !kaggle datasets download slothkong/10-monkey-species -p ./datasets/10-monkey-species\n",
        "  !unzip ./datasets/10-monkey-species/*.zip  -d ./datasets/10-monkey-species/ \n",
        "\n",
        "  DATASET_PATH = \"./datasets/10-monkey-species/\"\n",
        "  train_data = tc.image_analysis.load_images(f\"{DATASET_PATH}/training/\", with_path=True)\n",
        "  validate_data = tc.image_analysis.load_images(f\"{DATASET_PATH}/validation/\", with_path=True)\n",
        "  train_data.materialize()\n",
        "  validate_data.materialize()\n",
        "  print(len(train_data))\n",
        "  print(len(validate_data))\n",
        "\n",
        "  merge_data = train_data.join(validate_data, how='outer')\n",
        "  merge_data.materialize()\n",
        "  merge_data['label'] = merge_data['path'].apply(lambda p: p.split('/')[-2])\n",
        "  merge_data.materialize()\n",
        "  print(len(merge_data))\n",
        "\n",
        "  #need to change time duo to complexty\n",
        "  df_all_data = merge_data.to_dataframe()\n",
        "  \n",
        "  df_all_data = balance_data(df_all_data)\n",
        "\n",
        "  #need to change time duo to complexty\n",
        "  df_all_data.label = pd.Categorical(df_all_data.label)\n",
        "  df_all_data['code'] = df_all_data.label.cat.codes\n",
        "  df_all_data = df_all_data.drop(columns=['label'])\n",
        "  df_all_data = df_all_data.rename(columns={\"code\": \"label\"})\n",
        "\n",
        "  #call to cast img function\n",
        "  list_img = cast_img(list(df_all_data['path']))\n",
        "\n",
        "  df_all_data = df_all_data.drop(columns=['path'])\n",
        "  df_all_data = df_all_data.drop(columns=['image'])\n",
        "\n",
        "  arr = np.array(list_img)\n",
        "  list_img = arr.tolist()\n",
        "  df_all_data['data'] = list_img\n",
        "\n",
        "  #reduce data\n",
        "\n",
        "  df_all_data = df_all_data.iloc[::2]\n",
        "  print(Counter(list(df_all_data['label'])))\n",
        "  print(len(df_all_data))\n",
        "  df_all_data.to_csv(\"monkey.csv\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYZ-GhYwFWuP"
      },
      "source": [
        "## get_fruits_data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nbf_NNgiOgH"
      },
      "source": [
        "def get_fruits_data():\n",
        "  print('fruits')\n",
        "  # monky dataset load and pre process \n",
        "  !mkdir ./datasets/360-fruits\n",
        "  # download the dataset from Kaggle and unzip it\n",
        "\n",
        "  !kaggle datasets download -d moltean/fruits -p ./datasets/360-fruits\n",
        "  !unzip ./datasets/360-fruits/*.zip  -d ./datasets/360-fruits/ \n",
        "  DATASET_PATH = \"./datasets/360-fruits/fruits-360/\"\n",
        "  train_data = tc.image_analysis.load_images(f\"{DATASET_PATH}/Training/\", with_path=True)\n",
        "  train_data['label'] = train_data['path'].apply(lambda p: p.split('/')[-2])\n",
        "  test_data = tc.image_analysis.load_images(f\"{DATASET_PATH}/Test/\", with_path=True)\n",
        "  test_data['label'] = test_data['path'].apply(lambda p: p.split('/')[-2])\n",
        "\n",
        "  train_data.materialize()\n",
        "  test_data.materialize()\n",
        "  print(len(train_data) + len(test_data))\n",
        "\n",
        "  merge_data = train_data.join(test_data, how='outer')\n",
        "  merge_data.materialize()\n",
        "  print(len(merge_data))\n",
        "\n",
        "  #need to change time duo to complexty\n",
        "  df_all_data = merge_data.to_dataframe()\n",
        "\n",
        "  df_all_data = balance_data(df_all_data)\n",
        "\n",
        "  #need to change time duo to complexty\n",
        "  df_all_data.label = pd.Categorical(df_all_data.label)\n",
        "  df_all_data['code'] = df_all_data.label.cat.codes\n",
        "  df_all_data = df_all_data.drop(columns=['label'])\n",
        "  df_all_data = df_all_data.rename(columns={\"code\": \"label\"})\n",
        "\n",
        "  #call to cast img function\n",
        "  list_img = cast_img(list(df_all_data['path']))\n",
        "\n",
        "  df_all_data = df_all_data.drop(columns=['path'])\n",
        "  df_all_data = df_all_data.drop(columns=['image'])\n",
        "\n",
        "  arr = np.array(list_img)\n",
        "  list_img = arr.tolist()\n",
        "\n",
        "  df_all_data['data'] = list_img\n",
        "  df_all_data = df_all_data.iloc[::3]\n",
        "  print(Counter(list(df_all_data['label'])))\n",
        "\n",
        "  list_range = list(range(30))\n",
        "  print(list_range)\n",
        "  df1 = df_all_data[df_all_data['label'].isin(list_range)]\n",
        "  print('fruits1')\n",
        "  print(Counter(list(df1['label'])))\n",
        "  df1.to_csv(\"fruits1New.csv\")\n",
        "\n",
        "  list_range = list(range(30,50))\n",
        "  print(list_range)\n",
        "  df2 = df_all_data[df_all_data['label'].isin(list_range)]\n",
        "  print('fruits2')\n",
        "  print(Counter(list(df2['label'])))\n",
        "  df2.to_csv(\"fruits2New.csv\")\n",
        "\n",
        "  list_range = list(range(50,65))\n",
        "  print(list_range)\n",
        "  df3 = df_all_data[df_all_data['label'].isin(list_range)]\n",
        "  print('fruits3')\n",
        "  print(Counter(list(df3['label'])))\n",
        "  df3.to_csv(\"fruits3New.csv\")\n",
        "\n",
        "  list_range = list(range(65,75))\n",
        "  print(list_range)\n",
        "  df4 = df_all_data[df_all_data['label'].isin(list_range)]\n",
        "  print('fruits4')\n",
        "  print(Counter(list(df4['label'])))\n",
        "  df4.to_csv(\"fruits4New.csv\")\n",
        "\n",
        "  # df_all_data.to_csv(\"fruits.csv\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT_RkaZZFab8"
      },
      "source": [
        "## get_flowers_data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5enPiPmS_oev"
      },
      "source": [
        "def get_flowers_data():\n",
        "  print('flowers')\n",
        "  #dataset load and pre process \n",
        "  !mkdir ./datasets/flowers\n",
        "  # download the dataset from Kaggle and unzip it\n",
        "\n",
        "  !kaggle datasets download -d alxmamaev/flowers-recognition -p ./datasets/flowers\n",
        "  !unzip ./datasets/flowers/*.zip  -d ./datasets/flowers/ \n",
        "  DATASET_PATH = \"./datasets/flowers/\"\n",
        "\n",
        "  all_data = tc.image_analysis.load_images(f\"{DATASET_PATH}/\", with_path=True)\n",
        "  all_data['label'] = all_data['path'].apply(lambda p: p.split('/')[-2])\n",
        "\n",
        "  all_data.materialize()\n",
        "  print(all_data)\n",
        "\n",
        "  df_all_data = all_data.to_dataframe()\n",
        "  df_all_data = balance_data(df_all_data)\n",
        "\n",
        "  #need to change time duo to complexty\n",
        "  df_all_data.label = pd.Categorical(df_all_data.label)\n",
        "  df_all_data['code'] = df_all_data.label.cat.codes\n",
        "  df_all_data = df_all_data.drop(columns=['label'])\n",
        "  df_all_data = df_all_data.rename(columns={\"code\": \"label\"})\n",
        "\n",
        "  #call to cast img function\n",
        "  list_img = cast_img(list(df_all_data['path']))\n",
        "\n",
        "  df_all_data = df_all_data.drop(columns=['path'])\n",
        "  df_all_data = df_all_data.drop(columns=['image'])\n",
        "\n",
        "  arr = np.array(list_img)\n",
        "  list_img = arr.tolist()\n",
        "\n",
        "  df_all_data['data'] = list_img\n",
        "  print(len(df_all_data))\n",
        "\n",
        "  df_all_data = df_all_data.iloc[::10]\n",
        "  print(Counter(list(df_all_data['label'])))\n",
        "\n",
        "  df_all_data.to_csv(\"flowers.csv\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in0dq2x8SsDM"
      },
      "source": [
        "## get_butterfly_data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVLhBSw7PkeF"
      },
      "source": [
        "def get_butterfly_data():\n",
        "  print(\"butterfly\")\n",
        "  #dataset load and pre process \n",
        "  !mkdir ./datasets/butterfly\n",
        "  # download the dataset from Kaggle and unzip it\n",
        "\n",
        "  !kaggle datasets download -d gpiosenka/butterfly-images40-species -p ./datasets/butterfly\n",
        "  !unzip ./datasets/butterfly/*.zip  -d ./datasets/butterfly/ \n",
        "  DATASET_PATH = \"./datasets/butterfly/butterflies/\"\n",
        "  train_data = tc.image_analysis.load_images(f\"{DATASET_PATH}/train/\", with_path=True)\n",
        "  train_data['label'] = train_data['path'].apply(lambda p: p.split('/')[-2])\n",
        "  test_data = tc.image_analysis.load_images(f\"{DATASET_PATH}/test/\", with_path=True)\n",
        "  test_data['label'] = test_data['path'].apply(lambda p: p.split('/')[-2])\n",
        "  valid_data = tc.image_analysis.load_images(f\"{DATASET_PATH}/valid/\", with_path=True)\n",
        "  valid_data['label'] = valid_data['path'].apply(lambda p: p.split('/')[-2])\n",
        "\n",
        "  \n",
        "  train_data.materialize()\n",
        "  test_data.materialize()\n",
        "  valid_data.materialize()\n",
        "  print(len(train_data) + len(test_data) + len(valid_data))\n",
        "\n",
        "  merge_data = train_data.join(test_data, how='outer')\n",
        "  merge_data.materialize()\n",
        "  merge_data = merge_data.join(valid_data, how='outer')\n",
        "  merge_data.materialize()\n",
        "  print(len(merge_data))\n",
        "\n",
        "\n",
        "  #need to change time duo to complexty\n",
        "  df_all_data = merge_data.to_dataframe()\n",
        "\n",
        "  df_all_data = balance_data(df_all_data)\n",
        "\n",
        "  #need to change time duo to complexty\n",
        "  df_all_data.label = pd.Categorical(df_all_data.label)\n",
        "  df_all_data['code'] = df_all_data.label.cat.codes\n",
        "  df_all_data = df_all_data.drop(columns=['label'])\n",
        "  df_all_data = df_all_data.rename(columns={\"code\": \"label\"})\n",
        "\n",
        "  #call to cast img function\n",
        "  list_img = cast_img(list(df_all_data['path']))\n",
        "\n",
        "  df_all_data = df_all_data.drop(columns=['path'])\n",
        "  df_all_data = df_all_data.drop(columns=['image'])\n",
        "\n",
        "  arr = np.array(list_img)\n",
        "  list_img = arr.tolist()\n",
        "\n",
        "  df_all_data['data'] = list_img\n",
        "  print(Counter(list(df_all_data['label'])))\n",
        "\n",
        "  list_range = [0,1,2,3,4]\n",
        "  df1 = df_all_data[df_all_data['label'].isin(list_range)]\n",
        "  print('butterfly1')\n",
        "  print(Counter(list(df1['label'])))\n",
        "  print(len(df1))\n",
        "  df1.to_csv(\"butterfly1.csv\")\n",
        "\n",
        "  list_range = [5,15,20,25]\n",
        "  df2 = df_all_data[df_all_data['label'].isin(list_range)]\n",
        "  print('butterfly2')\n",
        "  print(Counter(list(df2['label'])))\n",
        "  print(len(df2))\n",
        "  df2.to_csv(\"butterfly2.csv\")\n",
        "\n",
        "  list_range = [30,40,49]\n",
        "  df3 = df_all_data[df_all_data['label'].isin(list_range)]\n",
        "  print('butterfly3')\n",
        "  print(Counter(list(df3['label'])))\n",
        "  print(len(df3))\n",
        "  df3.to_csv(\"butterfly3.csv\")\n",
        "\n",
        "  df_all_data.to_csv(\"butterfly.csv\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw9_fHn1_8gQ"
      },
      "source": [
        "## get_intel_data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYx2GfMEYpVb"
      },
      "source": [
        "def get_intel_data():\n",
        "  print(\"intel\")\n",
        "  #dataset load and pre process \n",
        "  !mkdir ./datasets/intel\n",
        "  # download the dataset from Kaggle and unzip it\n",
        "\n",
        "  !kaggle datasets download -d puneet6060/intel-image-classification -p ./datasets/intel\n",
        "  !unzip ./datasets/intel/*.zip  -d ./datasets/intel/ \n",
        "  DATASET_PATH = \"./datasets/intel/\"\n",
        "  train_data = tc.image_analysis.load_images(f\"{DATASET_PATH}/seg_train/seg_train/\", with_path=True)\n",
        "  train_data['label'] = train_data['path'].apply(lambda p: p.split('/')[-2])\n",
        "  test_data = tc.image_analysis.load_images(f\"{DATASET_PATH}/seg_test/seg_test/\", with_path=True)\n",
        "  test_data['label'] = test_data['path'].apply(lambda p: p.split('/')[-2])\n",
        "\n",
        "  train_data.materialize()\n",
        "  test_data.materialize()\n",
        "\n",
        "  merge_data = train_data.join(test_data, how='outer')\n",
        "  merge_data.materialize()\n",
        "\n",
        "  #need to change time duo to complexty\n",
        "  df_all_data = merge_data.to_dataframe()\n",
        "\n",
        "  df_all_data = balance_data(df_all_data)\n",
        "\n",
        "  #need to change time duo to complexty\n",
        "  df_all_data.label = pd.Categorical(df_all_data.label)\n",
        "  df_all_data['code'] = df_all_data.label.cat.codes\n",
        "  df_all_data = df_all_data.drop(columns=['label'])\n",
        "  df_all_data = df_all_data.rename(columns={\"code\": \"label\"})\n",
        "\n",
        "  #call to cast img function\n",
        "  list_img = cast_img(list(df_all_data['path']))\n",
        "\n",
        "  df_all_data = df_all_data.drop(columns=['path'])\n",
        "  df_all_data = df_all_data.drop(columns=['image'])\n",
        "\n",
        "  arr = np.array(list_img)\n",
        "  list_img = arr.tolist()\n",
        "\n",
        "  df_all_data['data'] = list_img\n",
        "\n",
        "  df_all_data = df_all_data.iloc[::10]\n",
        "  df_all_data = df_all_data.iloc[::2] \n",
        "  print(Counter(list(df_all_data['label'])))\n",
        "\n",
        "  df_all_data.to_csv(\"intel.csv\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FspZ4NzOUYT-"
      },
      "source": [
        "def data_csv_kaggle(flag):\n",
        "  if flag == 'monkey':\n",
        "    get_monkey_data()\n",
        "  if flag == 'fruits':\n",
        "    get_fruits_data()\n",
        "  if flag == 'flowers':\n",
        "    get_flowers_data()\n",
        "  if flag == 'butterfly':\n",
        "    get_butterfly_data()\n",
        "  if flag == 'intel':\n",
        "    get_intel_data()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtTpHtfkUvGW"
      },
      "source": [
        "list_data = ['monkey','fruits','flowers','butterfly','intel']\n",
        "for name in list_data:\n",
        "  data_csv_kaggle(name)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}